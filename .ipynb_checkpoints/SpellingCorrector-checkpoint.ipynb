{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correcting | NLP\n",
    "This notebook contains the implementation of a simple spelling corrector, based on Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "We formulate spelling correction as an classification problem and use Bayesian inference to obtain results. Although the basic problem formulation is straightforward, there is plenty room to improve the here developed models.\n",
    "\n",
    "Given a word $w$, find the most likely correction $c^{(k)}$ from the set of all correction candidates $C_w$ for $w$:\n",
    "\n",
    "        c = correct(w)\n",
    "\n",
    "We can formalize this problem in the following way:\n",
    "\n",
    "\\begin{align}\n",
    "argmax_{\\;k} \\; P(c^{(k)} | w)\n",
    "\\end{align}\n",
    "\n",
    "By using Bayes' theorem, we get:\n",
    "\\begin{align}\n",
    "argmax_{\\;k} \\; P(c^{(k)} | w) =  argmax_{\\;k} \\; \\frac{P(w | c^{(k)}) P(c^{(k)})}{P(w)} = argmax_{\\;k} P(w | c^{(k)}) P(c^{(k)})\\\\\n",
    "\\end{align}\n",
    "\n",
    "Further explanation:\n",
    "- We call $P(c^{(k)})$ the language model.\n",
    "- The set of Candidates $C_w$ for a given word $w$ needs to be calculated and requires some \"model\" of typographical errors.\n",
    "- We call $P(w | c^{(k)})$. the error Model or noise Channel model.\n",
    "\n",
    "You can find a more detailed explanation of this topic in chapter 5 of the book \"Speech and Language Processing\" by Daniel Jurafsky & James H. Martin.\n",
    "\n",
    "## Data\n",
    "In the git repository you can find the following documents:\n",
    "- data.txt\n",
    "- addconfusion.data\n",
    "- delconfusion.data\n",
    "- revconfusion.data\n",
    "- subconfusion.data\n",
    "- validate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "There are multiple classes of language models. The most simple ones are the unigram models, where the probability of each word is fixed and can be obtained by using the frequency of its occurrence in a given corpus.\n",
    "\n",
    "In the file `data.txt` there is the Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
    "by Sir Arthur Conan Doyle. This is used to train the language model.\n",
    "\n",
    "The next code implements a language-model function for $P(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 0.08710647079332473\n",
      "of: 0.04804613514506752\n",
      "and: 0.031619912742760756\n",
      "in: 0.02505017762407772\n",
      "to: 0.023821414773423293\n",
      "a: 0.020649623550031568\n",
      "is: 0.012423320109684046\n",
      "it: 0.009933756113189413\n",
      "that: 0.008531609547411965\n",
      "by: 0.008177303694769277\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#Create counter of all words\n",
    "with open(\"data.txt\") as f:\n",
    "        text_string = f.read().lower()\n",
    "        match_pattern = re.findall(r'\\b[a-z]{1,25}\\b', text_string)\n",
    "        wordcount = Counter(match_pattern)\n",
    "\n",
    "#Number of all counted words\n",
    "numberOfWords = sum(wordcount.values())\n",
    "        \n",
    "#Ten most frequent words  \n",
    "for word, count in wordcount.most_common(10):\n",
    "    print(\"{0}: {1}\".format(word, count/numberOfWords))\n",
    "\n",
    "def languageModel(word):\n",
    "    return(wordcount[word]/numberOfWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates\n",
    "We want to detect real-world-spelling-errors: This can happen from typographical errors, (insertion, deletion, transposition), or cognitive errors where the writer substituted the wrong spelling of a homophone or near-homophone (e.g., dessert for desert, or piece for peace).\n",
    "\n",
    "Thus, we focus on errors that were produced by the following operations:\n",
    "   - transposition (eg. \"caress\" for \"actress\" : \"ac\" -> \"ca\")\n",
    "   - deletion (\"acress\" for \"actress\": missing \"t\")\n",
    "   - substitutions (\"acress\" for \"access\": substituted \"r\" for \"c\")\n",
    "   - insertions (\"actresss\" for \"actress\": added \"s\")\n",
    "\n",
    "Given a word $w$, we create a set of possible candidates $C_w$ by applying all the possible operators on $w$. By concatenating operations, you can transform each word into any other arbitrary word. Yet, to keep things simply we only consider candidates $c^{(k)}$ with only one edit distance away from $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The subset of `words` that occur in our corpus\n",
    "def known(words): \n",
    "    return set(w for w in words if w in wordcount)\n",
    "\n",
    "def edits(word):\n",
    "    letters        = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits         = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    transposes     = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    deletes        = [L + R[1:]               for L, R in splits if R]\n",
    "    substitutions  = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts        = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(transposes + deletes + substitutions + inserts)\n",
    "\n",
    "def candidates(word): \n",
    "    return(known([word]) or known(edits(word)) or [word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Model\n",
    "Chapter 5  of \"Speech and Language Processing\" describes the noise channel model. Before we implement that, let us consider an even simpler error model where:\n",
    "\\begin{align}\n",
    "    P(w | c^{(k)}) = P(c^{(k)})\n",
    "\\end{align}\n",
    "\n",
    "Additionally for the first correction function we assume:\n",
    "- If $w$ is a known word, then $P(w) > P(c^{(k)})$ and therefore return $w$.\n",
    "- If $w$ is unknown  $argmax_k P(c^{(k)})$ and return $c^{(k)}$.\n",
    "- If both $w$ and its corrections are unknown, return $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word): \n",
    "    return (max(candidates(word), key=languageModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The file `validate.csv` contains a list of evaluation data of the form:\n",
    "\n",
    "    \"right, misspelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate: 0.7486033519553073\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('validate.csv', sep=',',names =[\"Correct\", \"Misspelled\"])\n",
    "data_points = [(data.iloc[i,0],data.iloc[i,1]) for i in range(len(data))]\n",
    "\n",
    "success = len([misspelled for correct, misspelled in data_points if correct==correction(misspelled)])\n",
    "print(\"success rate: {0}\".format(success/len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Noise Channel Model\n",
    "\n",
    "By analyzing the errors, you find examples where the corrected word is indeed a real word, but not the expected one. This phenomena occurs due to the simplistic nature of our language model. Unigram models do not include context sensitive information. Thus, improving you language model should naturally improve your validation results.\n",
    "\n",
    "Chapter 5. - \"Speech and Language Processing\" present another possibility to raise the classification success rate, by improving the error model. Instead of treating each error equally, we now consider the probability of on error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open('addconfusion.data', 'r') as file:\n",
    "    addmatrix=data=ast.literal_eval(file.read())\n",
    "with open('subconfusion.data', 'r') as file:\n",
    "    submatrix=data=ast.literal_eval(file.read())\n",
    "with open('revconfusion.data', 'r') as file:\n",
    "    revmatrix=data=ast.literal_eval(file.read())\n",
    "with open('delconfusion.data', 'r') as file:\n",
    "    delmatrix=data=ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate: 0.7932960893854749\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "### Zur Berechnung von P(c^(k)) wird das Dictionary angelegt, die Summe aller Wörter berechnet und die Funktion P(word) bereitgestellt\n",
    "with open(\"data.txt\") as f:\n",
    "        text_string = f.read().lower()\n",
    "        match_pattern = re.findall(r'\\b[a-z]{1,25}\\b', text_string)\n",
    "        wordcount = Counter(match_pattern)\n",
    "\n",
    "numberOfWords = sum(wordcount.values())\n",
    "\n",
    "def P(word):\n",
    "    return(wordcount[word]/numberOfWords)\n",
    "\n",
    "def known(words): \n",
    "    return set((w,p) for w,p in words if w in wordcount)\n",
    "\n",
    "### Die passenden edits eines Wortes werden immer direkt als Tupel mit der Wahrscheinlichkeit, dass dieser Fehler auftritt, gespeichert\n",
    "def edits(word):\n",
    "    letters        = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits         = [(word[:i], word[i:])                                                 for i in range(len(word) + 1)]\n",
    "    transposes     = [(L+R[1]+R[0]+R[2:], revmatrix[R[1]+R[0]]/sum(revmatrix.values()))    for L, R in splits if len(R)>1]\n",
    "    #Es wird kein edit mit dem gelöschten ersten Buchstaben erstellt, da es zu einem hinzugefügten Buchstaben ohne vorangehenden Buchstaben keine Werte gibt\n",
    "    deletes        = [(L+R[1:],           addmatrix[L[-1]+R[0]]/sum(addmatrix.values()))   for L, R in splits if R if L]\n",
    "    substitutions  = [(L+c+R[1:],         submatrix[c+R[0]]/sum(submatrix.values()))       for L, R in splits if R for c in letters]\n",
    "    #Auch bei insert wird kein edit mit einem am Anfang eingesetzten Buchstaben erstellt\n",
    "    inserts        = [(L+c+R,             delmatrix[L[-1]+c]/sum(delmatrix.values()))      for L, R in splits if L for c in letters]\n",
    "    return set(transposes + deletes + substitutions + inserts)\n",
    "\n",
    "def candidates(word): \n",
    "    return(known(edits(word)))\n",
    "\n",
    "def noiseChannelModel(word):\n",
    "    return(word[1]*P(word[0]))\n",
    "\n",
    "def correction(word): \n",
    "    cand = candidates(word)\n",
    "    if word in wordcount:\n",
    "        return word\n",
    "    elif cand:\n",
    "        return (max(cand, key=noiseChannelModel)[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('validate.csv', sep=',',names =[\"Correct\", \"Misspelled\"])\n",
    "data_points = [(data.iloc[i,0],data.iloc[i,1]) for i in range(len(data))]\n",
    "\n",
    "success = len([misspelled for correct, misspelled in data_points if correct==correction(misspelled)])\n",
    "print(\"success rate: {0}\".format(success/len(data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
